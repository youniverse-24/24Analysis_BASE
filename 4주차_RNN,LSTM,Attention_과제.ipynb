{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1CU08wPie0g"
   },
   "source": [
    "# ğŸ“– ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ Inductive Bias & NSMC ê°ì • ë¶„ì„ ì‹¤ìŠµ\n",
    "\n",
    "## âœ… **TODO: ê³¼ì œ 1 - Inductive Bias íƒêµ¬**\n",
    "- [ ] FC Layer, RNN, LSTM, Attentionì˜ Inductive Bias ì¡°ì‚¬  \n",
    "- [ ] ê°œì¸ì˜ í•´ì„ì„ ì •ë¦¬í•˜ì—¬ ê¸°ì…\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… **TODO: ê³¼ì œ 2 - NSMC ê°ì • ë¶„ì„ ì‹¤ìŠµ** (â—ë¹ˆì¹¸ ì±„ìš°ê¸° ì‹¤ìŠµ í¬í•¨ )  \n",
    "- [ ] NSMC ë°ì´í„° ì „ì²˜ë¦¬ (í† í°í™”, ì–´íœ˜ ì‚¬ì „ ìƒì„±)  \n",
    "- [ ] RNN, LSTM, Attention ëª¨ë¸ êµ¬í˜„ **(ë¹ˆì¹¸ ì±„ìš°ê¸°!)**  \n",
    "- [ ] ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ (ì •í™•ë„ ë¹„êµ, Confusion Matrix ë¶„ì„)  \n",
    "- [ ] ì‚¬ìš©ì ì…ë ¥ ë¬¸ì¥ ê°ì • ë¶„ì„  \n",
    "\n",
    "ğŸ“Œ **Colab ì‹¤í–‰ ì‹œ GPU (`T4`) ì‚¬ìš© ê¶Œì¥!**\n",
    "\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8prTLAi3XkSL"
   },
   "source": [
    "# ğŸ“– ê³¼ì œ 1 : FC Layer, RNN, LSTM, Attentionì˜ Inductive Bias íƒêµ¬\n",
    "\n",
    "## ğŸ“Œ ê³¼ì œ ê°œìš”  \n",
    "ì´ ê³¼ì œì—ì„œëŠ” ë”¥ëŸ¬ë‹ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” **Fully Connected (FC) Layer, RNN, LSTM, Attention**ì˜ **Inductive Bias**ë¥¼ ë¹„êµÂ·íƒêµ¬í•©ë‹ˆë‹¤.  \n",
    "Inductive Biasë€ **ëª¨ë¸ì´ í•™ìŠµí•  ë•Œ ì„ í˜¸í•˜ëŠ” íŒ¨í„´ì´ë‚˜ ì¶”ë¡  ë°©ì‹ì˜ íŠ¹ì„±ì„ ì˜ë¯¸**í•˜ë©°, ì„œë¡œ ë‹¤ë¥¸ ì‹ ê²½ë§ êµ¬ì¡°ì—ì„œ ì–´ë–»ê²Œ ë‹¤ë¥´ê²Œ ë‚˜íƒ€ë‚˜ëŠ”ì§€ ë¶„ì„í•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.  \n",
    "\n",
    "\n",
    "\n",
    "## ğŸ“‚  ê³¼ì œ ìš”êµ¬ì‚¬í•­  \n",
    "\n",
    "### ğŸ“Œ ëª¨ë¸ë³„ Inductive Bias ê°œë… ì¡°ì‚¬  \n",
    "ê° ëª¨ë¸ì´ ê°€ì§€ê³  ìˆëŠ” **Inductive Bias**ì— ëŒ€í•´ ì¡°ì‚¬í•˜ê³ , **ì´ë¡ ì  ë°°ê²½ì„ ì •ë¦¬**í•˜ì„¸ìš”.  \n",
    "ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì •ë¦¬í•˜ì„¸ìš”.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vv3aMSU5X-mW"
   },
   "source": [
    "**FC Layer**: Fully Connected LayerëŠ” ì–´ë–¤ inductive biasë¥¼ ê°€ì§€ë©°, ì–´ë–¤ íŒ¨í„´ì„ í•™ìŠµí•˜ëŠ”ê°€?  \n",
    "\n",
    "ë‹µë³€ : \n",
    "- Inductive Bias: ì…ë ¥ ë°ì´í„°ì˜ ëª¨ë“  íŠ¹ì„±ì´ ì„œë¡œ ë…ë¦½ì ì´ë©° ê´€ê³„ê°€ ì—†ë‹¤ê³  ê°€ì •í•¨.\n",
    "- í•™ìŠµí•˜ëŠ” íŒ¨í„´: ëª¨ë“  ë‰´ëŸ°ì´ ì„œë¡œ ì—°ê²°ë˜ì–´ ìˆì–´ íŠ¹ì •í•œ êµ¬ì¡°ì  ê´€ê³„ ì—†ì´ ë°ì´í„°ë¥¼ í•™ìŠµí•¨.\n",
    "  ë”°ë¼ì„œ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ë‹¤ì–‘í•œ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆì§€ë§Œ, ì—°ê´€ì„±ì´ ê°•í•œ êµ¬ì¡°ì  ë°ì´í„°ë¥¼ í•™ìŠµí•˜ëŠ” ë°ëŠ” ë¹„íš¨ìœ¨ì ì¼ ìˆ˜ ìˆìŒ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mIejc31YA21"
   },
   "source": [
    "**RNN**: ìˆœí™˜ êµ¬ì¡°ê°€ inductive biasì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ê°€?  \n",
    "\n",
    "ë‹µë³€ :\n",
    "- Inductive Bias: ìˆœì°¨ì ì¸ ë°ì´í„°(ì‹œí€€ìŠ¤ ë°ì´í„°)ì—ì„œ ì´ì „ ì‹œì ì˜ ì •ë³´ê°€ í˜„ì¬ ì‹œì ì˜ ì˜ˆì¸¡ì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤ê³  ê°€ì •í•¨.\n",
    "- ì˜í–¥: ì‹œê³„ì—´ ë°ì´í„°ë‚˜ ìì—°ì–´ ì²˜ë¦¬ì—ì„œ ë¬¸ë§¥ ì •ë³´ë¥¼ ê³ ë ¤í•  ìˆ˜ ìˆë„ë¡ í•¨.\n",
    "- í•˜ì§€ë§Œ ê¸´ ì‹œí€€ìŠ¤ì—ì„œëŠ” ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œ(vanishing gradient problem)ê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxmZeEMjYFi4"
   },
   "source": [
    "**LSTM**: ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì–´ë–¤ inductive biasê°€ ì¶”ê°€ë˜ëŠ”ê°€?  \n",
    "ë‹µë³€ :\n",
    "- Inductive Bias: ì¥ê¸° ì˜ì¡´ì„±ì„ íš¨ê³¼ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆë„ë¡ ê²Œì´íŠ¸ ë©”ì»¤ë‹ˆì¦˜(ì…ë ¥ ê²Œì´íŠ¸, ë§ê° ê²Œì´íŠ¸, ì¶œë ¥ ê²Œì´íŠ¸)ì„ ì¶”ê°€í•¨.\n",
    "- í•´ê²°í•˜ëŠ” ë¬¸ì œ: ê¸°ì¡´ RNNì˜ ì¥ê¸° ì˜ì¡´ì„± ë¬¸ì œë¥¼ í•´ê²°í•˜ì—¬ ê¸´ ë¬¸ë§¥ ì •ë³´ë„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ìœ ë„í•¨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0beew9RoYKoG"
   },
   "source": [
    "**Attention**: ë¬¸ì¥ ë‚´ íŠ¹ì • ë‹¨ì–´ì— ì§‘ì¤‘í•˜ëŠ” Attention ë©”ì»¤ë‹ˆì¦˜ì˜ inductive biasëŠ” ì–´ë–¤ íŠ¹ì§•ì´ ìˆëŠ”ê°€?\n",
    "\n",
    "ë‹µë³€ : \n",
    "- Inductive Bias: ëª¨ë“  ì…ë ¥ í† í°ì´ ë™ì¼í•œ ì¤‘ìš”ë„ë¥¼ ê°€ì§€ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, íŠ¹ì • í† í°ì´ ë” ì¤‘ìš”í•œ ì—­í• ì„ í•  ìˆ˜ ìˆë‹¤ê³  ê°€ì •í•¨.\n",
    "- íŠ¹ì§•: ë¬¸ì¥ ë‚´ì—ì„œ íŠ¹ì • ë‹¨ì–´ë‚˜ ë¬¸ë§¥ì´ ë” ì¤‘ìš”í•œ ê²½ìš° ê°€ì¤‘ì¹˜ë¥¼ ë†’ì—¬ í•™ìŠµí•˜ë©°, ì´ëŠ” ê¸°ê³„ ë²ˆì—­, ë¬¸ë§¥ ì´í•´, ë¬¸ì„œ ìš”ì•½ ë“±ì—ì„œ íš¨ê³¼ì ì„.\n",
    "- Transformer ëª¨ë¸ì—ì„œ ê°•ë ¥í•œ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ëŠ” ì´ìœ  ì¤‘ í•˜ë‚˜."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z49LCildZuwn"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcGXdJ_WYcue"
   },
   "source": [
    "# ğŸ“– ê³¼ì œ 2 : ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ê°ì • ë¶„ì„ (NSMC) ì‹¤ìŠµ  \n",
    "\n",
    "## ğŸ“Œ 1. ì‹¤ìŠµ ê°œìš”  \n",
    "ì´ ì‹¤ìŠµì—ì„œëŠ” **ë„¤ì´ë²„ ì˜í™” ë¦¬ë·° ê°ì • ë¶„ì„ ë°ì´í„°(NSMC)**ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ê°ì •ì„ ë¶„ë¥˜í•˜ëŠ” **ë”¥ëŸ¬ë‹ ëª¨ë¸(RNN, LSTM, RNN+Attention)**ì„ í•™ìŠµí•©ë‹ˆë‹¤.  \n",
    "ê° ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¹„êµí•˜ê³ , **ì‚¬ìš©ì ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•œ ê°ì • ì˜ˆì¸¡**ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.  \n",
    "\n",
    "\n",
    "## ğŸ¯ 2. ì‹¤ìŠµ ëª©í‘œ  \n",
    "âœ” ê°ì • ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì „ì²˜ë¦¬ ë° ëª¨ë¸ í•™ìŠµ ì´í•´  \n",
    "âœ” RNN, LSTM, Attentionì„ í™œìš©í•œ ê°ì • ë¶„ì„ ëª¨ë¸ êµ¬í˜„ ë° ì„±ëŠ¥ ë¹„êµ  \n",
    "âœ” í•™ìŠµí•œ ëª¨ë¸ì´ ì‹¤ì œ ë¬¸ì¥ì„ ì–´ë–»ê²Œ ë¶„ë¥˜í•˜ëŠ”ì§€ í™•ì¸  \n",
    "\n",
    "### GPU ì‚¬ìš© ê¶Œì¥! (ex, colabì˜ T4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRW_OwxMRCPR"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 0. ê¸°ë³¸ ì„¤ì¹˜ (Colab í™˜ê²½)\n",
    "############################\n",
    "\n",
    "# Colab ë“±ì—ì„œ í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° ì‚¬ìš© ìœ„í•´ konlpy ì„¤ì¹˜\n",
    "\n",
    " !pip install konlpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6w0uVwjURDFF"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
    "############################\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# í•œêµ­ì–´ í˜•íƒœì†Œ ë¶„ì„ê¸° (ì˜ˆ: Okt ì‚¬ìš©)\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Luv3lW5SRG3e"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 2. NSMC ë°ì´í„° ë‹¤ìš´ë¡œë“œ\n",
    "############################\n",
    "\n",
    "# ratings_train.txt, ratings_test.txt íŒŒì¼ì´ ì—†ë‹¤ë©´ ë‹¤ìš´ë¡œë“œ\n",
    "# (Colab í™˜ê²½ì—ì„œ ì˜ˆì‹œ)\n",
    "if not os.path.isfile(\"ratings_train.txt\"):\n",
    "    url_train = \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\"\n",
    "    urllib.request.urlretrieve(url_train, \"ratings_train.txt\")\n",
    "\n",
    "if not os.path.isfile(\"ratings_test.txt\"):\n",
    "    url_test = \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\"\n",
    "    urllib.request.urlretrieve(url_test, \"ratings_test.txt\")\n",
    "\n",
    "print(\"NSMC ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì™„ë£Œ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MRBJyVHRIqH"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 3. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬\n",
    "############################\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    ratings_train.txt / ratings_test.txt íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ì„œ\n",
    "    [text, label] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        next(f)  # í—¤ë” ê±´ë„ˆë›°ê¸°\n",
    "        for line in f:\n",
    "            id_, text, label = line.strip().split('\\t')\n",
    "            if text == \"\":  # ê³µë°± ë¦¬ë·° ì œê±°\n",
    "                continue\n",
    "            data.append((text, int(label)))\n",
    "    return data\n",
    "\n",
    "train_data_raw = load_data(\"ratings_train.txt\")\n",
    "test_data_raw = load_data(\"ratings_test.txt\")\n",
    "\n",
    "print(\"í›ˆë ¨ ë°ì´í„° ê°œìˆ˜:\", len(train_data_raw))\n",
    "print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ê°œìˆ˜:\", len(test_data_raw))\n",
    "\n",
    "# ë°ì´í„° ìƒ˜í”Œ í™•ì¸\n",
    "print(\"\\nì²« ë²ˆì§¸ í›ˆë ¨ ìƒ˜í”Œ:\", train_data_raw[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIVZhY5dRKPi"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 3-1. í˜•íƒœì†Œ ë¶„ì„ & í† í°í™”\n",
    "############################\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    ê°„ë‹¨íˆ Okt í˜•íƒœì†Œ ë¶„ì„ê¸°ë¡œ ëª…ì‚¬, ë™ì‚¬ ë“± ë‹¨ì–´ ë‹¨ìœ„ í† í°í™” ì˜ˆì‹œ.\n",
    "    í•„ìš”ì— ë”°ë¼ ë¶ˆìš©ì–´ ì œê±° ë“±ì˜ ì¶”ê°€ ì²˜ë¦¬ê°€ ê°€ëŠ¥.\n",
    "    \"\"\"\n",
    "    # ì˜ˆì‹œ: ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ì œê±° (ì •ê·œ í‘œí˜„ì‹ ë“±)\n",
    "    # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±, ì¼ë¶€ ë¬¸ì¥ë¶€í˜¸ë§Œ ë‚¨ê¸´ë‹¤ê³  ê°€ì •\n",
    "    import re\n",
    "    text = re.sub(r\"[^ê°€-í£0-9a-zA-Z\\s?!.,]\", \"\", text)\n",
    "\n",
    "    # í˜•íƒœì†Œ ë¶„ì„(Okt)\n",
    "    # ì´ ì˜ˆì‹œì—ì„  ê·¸ëƒ¥ morphemesë§Œ ì¶”ì¶œ\n",
    "    tokens = okt.morphs(text.strip())\n",
    "    return tokens\n",
    "\n",
    "# ê°„ë‹¨ í…ŒìŠ¤íŠ¸\n",
    "sample_text = train_data_raw[0][0]\n",
    "print(\"ì›ë¬¸:\", sample_text)\n",
    "print(\"í† í°í™” ê²°ê³¼:\", tokenize(sample_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rv0mQD_JRL4A"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 3-2. ì–´íœ˜ ì‚¬ì „(Vocabulary) ìƒì„±\n",
    "############################\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# ë§¤ìš° í° ë°ì´í„°ë¥¼ ëª¨ë‘ ì‚¬ìš©í•  ê²½ìš° ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìœ¼ë¯€ë¡œ,\n",
    "# ìš°ì„  \"ì¼ë¶€\" ë°ì´í„°(ì˜ˆ: ìƒìœ„ Nê°œ)ë§Œ ì‚¬ìš©í•´ ì‹œë²”ì‹¤ìŠµí•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "# í•„ìš”ì‹œ full_data = train_data_raw ë¡œ ë³€ê²½í•˜ì„¸ìš”.\n",
    "N = 20000  # ì‹œë²”ìœ¼ë¡œ 2ë§Œ ê°œë§Œ ì‚¬ìš©í•  ê²½ìš°\n",
    "full_data = train_data_raw[:N]\n",
    "\n",
    "word_freq = defaultdict(int)\n",
    "\n",
    "for text, label in full_data:\n",
    "    tokens = tokenize(text)\n",
    "    for tok in tokens:\n",
    "        word_freq[tok] += 1\n",
    "\n",
    "# ì‚¬ìš© ë¹ˆë„ê°€ ë‚®ì€ ë‹¨ì–´ëŠ” ì œì™¸ (ì˜ˆ: 3íšŒ ë¯¸ë§Œ)\n",
    "min_freq = 3\n",
    "vocab = [\"<pad>\", \"<unk>\"]\n",
    "for w, f in word_freq.items():\n",
    "    if f >= min_freq:\n",
    "        vocab.append(w)\n",
    "\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "print(\"ë‹¨ì–´ ì‚¬ì „ í¬ê¸°:\", len(vocab))\n",
    "\n",
    "def text_to_ids(text, word2idx):\n",
    "    tokens = tokenize(text)\n",
    "    ids = []\n",
    "    for tok in tokens:\n",
    "        ids.append(word2idx.get(tok, word2idx[\"<unk>\"]))\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mf8dSkbrRNoR"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 4. Dataset ë° DataLoader êµ¬ì„±\n",
    "############################\n",
    "\n",
    "# ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´(íŒ¨ë”©ì„ ìœ„í•œ). ë„ˆë¬´ í¬ê²Œ ì¡ìœ¼ë©´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€\n",
    "max_len = 30\n",
    "\n",
    "def pad_sequences(seq, max_len):\n",
    "    \"\"\"\n",
    "    ì…ë ¥ëœ seq (List of token ids)ì— ëŒ€í•´\n",
    "    max_lenë§Œí¼ ë’¤ë¥¼ <pad> í† í°ìœ¼ë¡œ ì±„ì›Œë„£ê±°ë‚˜ ìë¥´ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    if len(seq) < max_len:\n",
    "        seq = seq + [word2idx[\"<pad>\"]] * (max_len - len(seq))\n",
    "    else:\n",
    "        seq = seq[:max_len]\n",
    "    return seq\n",
    "\n",
    "class NSMCDataset(Dataset):\n",
    "    def __init__(self, data, word2idx, max_len=30):\n",
    "        self.data = data\n",
    "        self.word2idx = word2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, label = self.data[idx]\n",
    "        token_ids = text_to_ids(text, self.word2idx)\n",
    "        token_ids = pad_sequences(token_ids, self.max_len)\n",
    "        return torch.LongTensor(token_ids), torch.LongTensor([label])\n",
    "\n",
    "# ì‹¤ì œë¡œëŠ” ì „ ë°ì´í„° ì“°ë ¤ë©´ full_data = train_data_raw, etc.\n",
    "# ì—¬ê¸°ì„œëŠ” ìœ„ì—ì„œ N=20000ìœ¼ë¡œ ì œí•œí•œ ë°ì´í„° ì‚¬ìš© (ì‹œê°„ ë‹¨ì¶•ìš©)\n",
    "train_dataset = NSMCDataset(full_data, word2idx, max_len=max_len)\n",
    "test_dataset = NSMCDataset(test_data_raw, word2idx, max_len=max_len)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"í›ˆë ¨ ë°°ì¹˜ ê°œìˆ˜: {len(train_loader)}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°°ì¹˜ ê°œìˆ˜: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0sH1Rzo9RO9b"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 4. Dataset ë° DataLoader êµ¬ì„±\n",
    "############################\n",
    "\n",
    "# ìµœëŒ€ ë¬¸ì¥ ê¸¸ì´(íŒ¨ë”©ì„ ìœ„í•œ). ë„ˆë¬´ í¬ê²Œ ì¡ìœ¼ë©´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€\n",
    "max_len = 30\n",
    "\n",
    "def pad_sequences(seq, max_len):\n",
    "    \"\"\"\n",
    "    ì…ë ¥ëœ seq (List of token ids)ì— ëŒ€í•´\n",
    "    max_lenë§Œí¼ ë’¤ë¥¼ <pad> í† í°ìœ¼ë¡œ ì±„ì›Œë„£ê±°ë‚˜ ìë¥´ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    if len(seq) < max_len:\n",
    "        seq = seq + [word2idx[\"<pad>\"]] * (max_len - len(seq))\n",
    "    else:\n",
    "        seq = seq[:max_len]\n",
    "    return seq\n",
    "\n",
    "class NSMCDataset(Dataset):\n",
    "    def __init__(self, data, word2idx, max_len=30):\n",
    "        self.data = data\n",
    "        self.word2idx = word2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, label = self.data[idx]\n",
    "        token_ids = text_to_ids(text, self.word2idx)\n",
    "        token_ids = pad_sequences(token_ids, self.max_len)\n",
    "        return torch.LongTensor(token_ids), torch.LongTensor([label])\n",
    "\n",
    "# ì‹¤ì œë¡œëŠ” ì „ ë°ì´í„° ì“°ë ¤ë©´ full_data = train_data_raw, etc.\n",
    "# ì—¬ê¸°ì„œëŠ” ìœ„ì—ì„œ N=20000ìœ¼ë¡œ ì œí•œí•œ ë°ì´í„° ì‚¬ìš© (ì‹œê°„ ë‹¨ì¶•ìš©)\n",
    "train_dataset = NSMCDataset(full_data, word2idx, max_len=max_len)\n",
    "test_dataset = NSMCDataset(test_data_raw, word2idx, max_len=max_len)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"í›ˆë ¨ ë°°ì¹˜ ê°œìˆ˜: {len(train_loader)}\")\n",
    "print(f\"í…ŒìŠ¤íŠ¸ ë°°ì¹˜ ê°œìˆ˜: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DcKYZHaRRIC"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 5. ëª¨ë¸ ì •ì˜\n",
    "#   5-1) RNN Classifier\n",
    "############################\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes=2):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(__________, __________, batch_first=True)  # (íŒíŠ¸: ì…ë ¥ ì°¨ì›ì€ ë‹¨ì–´ ì„ë² ë”© ì°¨ì›, ì€ë‹‰ ìƒíƒœ ì°¨ì›ì€ hidden_dim)\n",
    "        self.fc = nn.Linear(__________, __________)  # (íŒíŠ¸: ë§ˆì§€ë§‰ ì€ë‹‰ ìƒíƒœ í¬ê¸°ì™€ ì¶œë ¥ í´ë˜ìŠ¤ ê°œìˆ˜)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len)\n",
    "        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        output, hidden = self.rnn(__________)  # (íŒíŠ¸: RNNì˜ ì…ë ¥ì€ ì„ë² ë”©ëœ ë‹¨ì–´ ë²¡í„°)\n",
    "        last_hidden = hidden[__________]  # (íŒíŠ¸: RNNì˜ ë§ˆì§€ë§‰ ì€ë‹‰ ìƒíƒœë¥¼ ê°€ì ¸ì˜¤ê¸°, 0ë²ˆì§¸ ì¸ë±ìŠ¤ ì‚¬ìš©)\n",
    "        logits = self.fc(__________)  # (íŒíŠ¸: ìµœì¢… ì€ë‹‰ ìƒíƒœë¥¼ FC Layerì— í†µê³¼ì‹œì¼œ ê°ì • ì˜ˆì¸¡)\n",
    "        return logits\n",
    "\n",
    "############################\n",
    "#   5-2) LSTM Classifier\n",
    "############################\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes=2):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(__________, __________)  # (íŒíŠ¸: ë‹¨ì–´ ì‚¬ì „ í¬ê¸°ì™€ ì„ë² ë”© ì°¨ì› ì…ë ¥)\n",
    "        self.lstm = nn.LSTM(__________, __________, batch_first=True)  # (íŒíŠ¸: LSTMì˜ ì…ë ¥ ì°¨ì›ì€ embed_dim, ì¶œë ¥ ì°¨ì›ì€ hidden_dim)\n",
    "        self.fc = nn.Linear(__________, __________)  # (íŒíŠ¸: ë§ˆì§€ë§‰ ì€ë‹‰ ìƒíƒœ í¬ê¸°ì™€ ì¶œë ¥ í´ë˜ìŠ¤ ê°œìˆ˜)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        last_hidden = hidden[__________]  # (íŒíŠ¸: LSTMì˜ ë§ˆì§€ë§‰ hidden state ì‚¬ìš©, 0ë²ˆì§¸ ì¸ë±ìŠ¤)\n",
    "        logits = self.fc(__________)  # (íŒíŠ¸: ë§ˆì§€ë§‰ ì€ë‹‰ ìƒíƒœë¥¼ FC Layerì— í†µê³¼ì‹œì¼œ ê°ì • ì˜ˆì¸¡)\n",
    "        return logits\n",
    "\n",
    "############################\n",
    "#   5-3) RNN + Attention\n",
    "############################\n",
    "\n",
    "class RNNWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes=2):\n",
    "        super(RNNWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(__________, __________)  # (íŒíŠ¸: ë‹¨ì–´ ì‚¬ì „ í¬ê¸°ì™€ ì„ë² ë”© ì°¨ì› ì…ë ¥)\n",
    "        self.rnn = nn.RNN(__________, __________, batch_first=True)  # (íŒíŠ¸: RNNì˜ ì…ë ¥ ì°¨ì›ì€ embed_dim, ì¶œë ¥ ì°¨ì›ì€ hidden_dim)\n",
    "        self.attn_fc = nn.Linear(__________, __________)  # (íŒíŠ¸: attention ê°€ì¤‘ì¹˜ë¥¼ í•™ìŠµí•˜ëŠ” FC Layer, ì…ë ¥ì€ hidden_dim)\n",
    "        self.fc = nn.Linear(__________, __________)  # (íŒíŠ¸: ìµœì¢… context ë²¡í„°ë¥¼ ê°ì • ë¶„ë¥˜í•˜ëŠ” FC Layerì— ì…ë ¥)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        output, hidden = self.rnn(embedded)  # (batch, seq_len, hidden_dim)\n",
    "\n",
    "        # ìŠ¤ì½”ì–´ ê³„ì‚°\n",
    "        score = torch.tanh(self.attn_fc(__________))  # (íŒíŠ¸: outputì„ attention layerì— í†µê³¼)\n",
    "\n",
    "        # hidden_dim -> 1 ë¡œ ì¶•ì†Œí•˜ì—¬ ê° íƒ€ì„ìŠ¤í…ë³„ ìŠ¤ì¹¼ë¼ ìŠ¤ì½”ì–´ ì–»ê¸°\n",
    "        score = torch.sum(score, dim=__________)  # (íŒíŠ¸: hidden_dim ì°¨ì› ì¶•ì†Œ, dim=2)\n",
    "\n",
    "        # softmaxë¡œ ê°€ì¤‘ì¹˜ ê³„ì‚°\n",
    "        attn_weights = F.softmax(score, dim=__________).unsqueeze(-1)  # (íŒíŠ¸: ì‹œí€€ìŠ¤ ì°¨ì›ì— ëŒ€í•´ softmax ì ìš©, dim=1)\n",
    "\n",
    "        # ê°€ì¤‘í•©\n",
    "        context = output * attn_weights  # (batch, seq_len, hidden_dim)\n",
    "        context = torch.sum(context, dim=__________)  # (íŒíŠ¸: íƒ€ì„ìŠ¤í… ì°¨ì›ì„ ì¶•ì†Œí•˜ì—¬ ë¬¸ì¥ ë‹¨ìœ„ ë²¡í„° ì–»ê¸°, dim=1)\n",
    "\n",
    "        logits = self.fc(__________)  # (íŒíŠ¸: context ë²¡í„°ë¥¼ FC Layerì— ì…ë ¥í•˜ì—¬ ê°ì • ë¶„ë¥˜)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zEx46AkTRTS5"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 6. í•™ìŠµ / í‰ê°€ ë£¨í”„ ì •ì˜\n",
    "############################\n",
    "\n",
    "def train_model(model, train_loader, val_loader=None, epochs=5, lr=0.001):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)      # (batch, seq_len)\n",
    "            labels = labels.squeeze().to(device)  # (batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)         # (batch, num_classes)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # ì—í­ë³„ í‰ê·  ë¡œìŠ¤\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # ê²€ì¦ ì •í™•ë„(ì˜µì…˜)\n",
    "        if val_loader is not None:\n",
    "            acc = evaluate_model(model, val_loader)\n",
    "            val_accuracies.append(acc)\n",
    "            print(f\"[Epoch {epoch}] Train Loss: {epoch_loss:.4f}, Val Acc: {acc*100:.2f}%\")\n",
    "        else:\n",
    "            print(f\"[Epoch {epoch}] Train Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    return train_losses, val_accuracies\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (inputs, labels) in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.squeeze().to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy().tolist())\n",
    "            all_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    return acc\n",
    "\n",
    "def plot_training_curve(train_losses, val_accuracies=None, title=\"Training Curve\"):\n",
    "    plt.figure(figsize=(10,4))\n",
    "\n",
    "    # í•™ìŠµ ì†ì‹¤ ê³¡ì„ \n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.title(\"Train Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # ê²€ì¦ ì •í™•ë„ ê³¡ì„ \n",
    "    if val_accuracies is not None and len(val_accuracies) > 0:\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(val_accuracies, label='Val Acc', color='orange')\n",
    "        plt.title(\"Validation Accuracy\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79jQk5VHRU68"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 7. ë°ì´í„° ë¶„í• (Train/Valid) ì¤€ë¹„\n",
    "############################\n",
    "\n",
    "# ì—¬ê¸°ì„œëŠ” ë³„ë„ì˜ Validation ì„¸íŠ¸ë¥¼ ë§Œë“¤ì§€ ì•Šê³ ,\n",
    "# ì£¼ì–´ì§„ test_data_rawë¥¼ ìµœì¢… í…ŒìŠ¤íŠ¸ ìš©ë„ë¡œë§Œ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŒ.\n",
    "# í•˜ì§€ë§Œ ê°„ë‹¨íˆ train_data_rawë¥¼ ë‹¤ì‹œ train/valë¡œ ë‚˜ëˆ„ëŠ” ë°©ë²• ì‹œì—°.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# í˜„ì¬ full_data (N=20000) ì¤‘ 80%: train, 20%: val\n",
    "train_subset, val_subset = train_test_split(full_data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset_2 = NSMCDataset(train_subset, word2idx, max_len)\n",
    "val_dataset_2   = NSMCDataset(val_subset, word2idx, max_len)\n",
    "\n",
    "train_loader_2 = DataLoader(train_dataset_2, batch_size=batch_size, shuffle=True)\n",
    "val_loader_2   = DataLoader(val_dataset_2, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train subset: {len(train_subset)}, Val subset: {len(val_subset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dKDag3ORWYP"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 8. ëª¨ë¸ë³„ í•™ìŠµ & í‰ê°€\n",
    "############################\n",
    "\n",
    "embed_dim = 128\n",
    "hidden_dim = 128\n",
    "num_classes = 2\n",
    "EPOCHS = 20 # ìœ ë™ì ìœ¼ë¡œ ì¡°ì ˆí•˜ì„¸ìš”\n",
    "LR = 0.001\n",
    "\n",
    "# --- 8-1. RNN ---\n",
    "print(\"========= RNN Training =========\")\n",
    "rnn_model = RNNClassifier(len(vocab), embed_dim, hidden_dim, num_classes)\n",
    "rnn_train_losses, rnn_val_accs = train_model(rnn_model, train_loader_2, val_loader_2, epochs=EPOCHS, lr=LR)\n",
    "plot_training_curve(rnn_train_losses, rnn_val_accs, title=\"RNN\")\n",
    "\n",
    "# ìµœì¢… í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì •í™•ë„\n",
    "rnn_test_acc = evaluate_model(rnn_model, test_loader)\n",
    "print(f\"RNN Test Accuracy: {rnn_test_acc*100:.2f}%\\n\")\n",
    "\n",
    "# --- 8-2. LSTM ---\n",
    "print(\"========= LSTM Training =========\")\n",
    "lstm_model = LSTMClassifier(len(vocab), embed_dim, hidden_dim, num_classes)\n",
    "lstm_train_losses, lstm_val_accs = train_model(lstm_model, train_loader_2, val_loader_2, epochs=EPOCHS, lr=LR)\n",
    "plot_training_curve(lstm_train_losses, lstm_val_accs, title=\"LSTM\")\n",
    "\n",
    "# ìµœì¢… í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì •í™•ë„\n",
    "lstm_test_acc = evaluate_model(lstm_model, test_loader)\n",
    "print(f\"LSTM Test Accuracy: {lstm_test_acc*100:.2f}%\\n\")\n",
    "\n",
    "# --- 8-3. RNN + Attention ---\n",
    "print(\"========= RNN + Attention Training =========\")\n",
    "attn_model = RNNWithAttention(len(vocab), embed_dim, hidden_dim, num_classes)\n",
    "attn_train_losses, attn_val_accs = train_model(attn_model, train_loader_2, val_loader_2, epochs=EPOCHS, lr=LR)\n",
    "plot_training_curve(attn_train_losses, attn_val_accs, title=\"RNN+Attention\")\n",
    "\n",
    "# ìµœì¢… í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ì •í™•ë„\n",
    "attn_test_acc = evaluate_model(attn_model, test_loader)\n",
    "print(f\"RNN+Attention Test Accuracy: {attn_test_acc*100:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eUFqtZw8RYQn"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 9. í˜¼ë™ í–‰ë ¬(Confusion Matrix) ì‹œê°í™”\n",
    "############################\n",
    "\n",
    "def plot_confusion_matrix(model, data_loader, title=\"Confusion Matrix\"):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (inputs, labels) in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.squeeze().to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"=== Confusion Matrix: RNN ===\")\n",
    "plot_confusion_matrix(rnn_model, test_loader, title=\"RNN Confusion Matrix\")\n",
    "\n",
    "print(\"=== Confusion Matrix: LSTM ===\")\n",
    "plot_confusion_matrix(lstm_model, test_loader, title=\"LSTM Confusion Matrix\")\n",
    "\n",
    "print(\"=== Confusion Matrix: RNN+Attention ===\")\n",
    "plot_confusion_matrix(attn_model, test_loader, title=\"RNN+Attention Confusion Matrix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "En4Dc61WRZ7f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# ë¬¸ì¥ì„ ëª¨ë¸ì´ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ ë³€í™˜í•˜ê³  ê°ì • ë¶„ì„ ìˆ˜í–‰\n",
    "def predict_sentiment(model, sentence, word2idx, max_len=30):\n",
    "    \"\"\"\n",
    "    ê°ì • ë¶„ì„ì„ ìˆ˜í–‰í•˜ëŠ” í•¨ìˆ˜ (ì…ë ¥ ë¬¸ì¥ì„ ìˆ«ì í† í°ìœ¼ë¡œ ë³€í™˜ í›„ ëª¨ë¸ì— ì…ë ¥)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    tokens = tokenize(sentence)  # í˜•íƒœì†Œ ë¶„ì„ ë° í† í°í™”\n",
    "    token_ids = [word2idx.get(tok, word2idx[\"<unk>\"]) for tok in tokens]  # ë‹¨ì–´ë¥¼ IDë¡œ ë³€í™˜\n",
    "    token_ids = pad_sequences(token_ids, max_len)  # íŒ¨ë”© ì ìš©\n",
    "\n",
    "    input_tensor = torch.LongTensor(token_ids).unsqueeze(0).to(device)  # (1, seq_len)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)  # ëª¨ë¸ ì˜ˆì¸¡\n",
    "        probs = F.softmax(output, dim=1)  # í™•ë¥  ê°’\n",
    "        pred_label = torch.argmax(probs, dim=1).item()  # ì˜ˆì¸¡ ë¼ë²¨ (0: ë¶€ì •, 1: ê¸ì •)\n",
    "\n",
    "    return pred_label, probs.squeeze().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_nz7QRmZVOt"
   },
   "outputs": [],
   "source": [
    "# ì˜ˆì¸¡ í•¨ìˆ˜: 3ê°œ ëª¨ë¸ ëª¨ë‘ ì‹¤í–‰\n",
    "def predict_with_all_models(sentence):\n",
    "    \"\"\"\n",
    "    ì‚¬ìš©ì ì…ë ¥ ë¬¸ì¥ì— ëŒ€í•´ RNN, LSTM, RNN+Attention ëª¨ë¸ì´ ê°ê° ì–´ë–»ê²Œ ì˜ˆì¸¡í•˜ëŠ”ì§€ ì¶œë ¥\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ’¬ ì…ë ¥ ë¬¸ì¥:\", sentence)\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    models = {\n",
    "        \"RNN\": rnn_model,\n",
    "        \"LSTM\": lstm_model,\n",
    "        \"RNN+Attention\": attn_model\n",
    "    }\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        label, probs = predict_sentiment(model, sentence, word2idx)\n",
    "        sentiment = \"ê¸ì • ğŸ˜Š\" if label == 1 else \"ë¶€ì • ğŸ˜¡\"\n",
    "\n",
    "        print(f\"ğŸ§  [{model_name} ëª¨ë¸]\")\n",
    "        print(f\"ğŸ” ì˜ˆì¸¡ ê°ì •: {sentiment}\")\n",
    "        print(f\"ğŸ“Š í™•ë¥ : ë¶€ì • {probs[0]:.4f} | ê¸ì • {probs[1]:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°\n",
    "sample_sentence = input(\"ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "predict_with_all_models(sample_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì•ˆëŒì•„ê° ... "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
