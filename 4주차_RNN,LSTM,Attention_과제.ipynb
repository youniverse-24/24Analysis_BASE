{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1CU08wPie0g"
   },
   "source": [
    "# 📖 딥러닝 모델의 Inductive Bias & NSMC 감정 분석 실습\n",
    "\n",
    "## ✅ **TODO: 과제 1 - Inductive Bias 탐구**\n",
    "- [ ] FC Layer, RNN, LSTM, Attention의 Inductive Bias 조사  \n",
    "- [ ] 개인의 해석을 정리하여 기입\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ **TODO: 과제 2 - NSMC 감정 분석 실습** (❗빈칸 채우기 실습 포함 )  \n",
    "- [ ] NSMC 데이터 전처리 (토큰화, 어휘 사전 생성)  \n",
    "- [ ] RNN, LSTM, Attention 모델 구현 **(빈칸 채우기!)**  \n",
    "- [ ] 모델 학습 및 평가 (정확도 비교, Confusion Matrix 분석)  \n",
    "- [ ] 사용자 입력 문장 감정 분석  \n",
    "\n",
    "📌 **Colab 실행 시 GPU (`T4`) 사용 권장!**\n",
    "\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8prTLAi3XkSL"
   },
   "source": [
    "# 📖 과제 1 : FC Layer, RNN, LSTM, Attention의 Inductive Bias 탐구\n",
    "\n",
    "## 📌 과제 개요  \n",
    "이 과제에서는 딥러닝에서 자주 사용되는 **Fully Connected (FC) Layer, RNN, LSTM, Attention**의 **Inductive Bias**를 비교·탐구합니다.  \n",
    "Inductive Bias란 **모델이 학습할 때 선호하는 패턴이나 추론 방식의 특성을 의미**하며, 서로 다른 신경망 구조에서 어떻게 다르게 나타나는지 분석하는 것이 목표입니다.  \n",
    "\n",
    "\n",
    "\n",
    "## 📂  과제 요구사항  \n",
    "\n",
    "### 📌 모델별 Inductive Bias 개념 조사  \n",
    "각 모델이 가지고 있는 **Inductive Bias**에 대해 조사하고, **이론적 배경을 정리**하세요.  \n",
    "다음 질문에 대한 답변을 정리하세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vv3aMSU5X-mW"
   },
   "source": [
    "**FC Layer**: Fully Connected Layer는 어떤 inductive bias를 가지며, 어떤 패턴을 학습하는가?  \n",
    "\n",
    "답변 : \n",
    "- Inductive Bias: 입력 데이터의 모든 특성이 서로 독립적이며 관계가 없다고 가정함.\n",
    "- 학습하는 패턴: 모든 뉴런이 서로 연결되어 있어 특정한 구조적 관계 없이 데이터를 학습함.\n",
    "  따라서 충분한 데이터가 있다면 다양한 패턴을 학습할 수 있지만, 연관성이 강한 구조적 데이터를 학습하는 데는 비효율적일 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mIejc31YA21"
   },
   "source": [
    "**RNN**: 순환 구조가 inductive bias에 어떤 영향을 미치는가?  \n",
    "\n",
    "답변 :\n",
    "- Inductive Bias: 순차적인 데이터(시퀀스 데이터)에서 이전 시점의 정보가 현재 시점의 예측에 영향을 미친다고 가정함.\n",
    "- 영향: 시계열 데이터나 자연어 처리에서 문맥 정보를 고려할 수 있도록 함.\n",
    "- 하지만 긴 시퀀스에서는 장기 의존성 문제(vanishing gradient problem)가 발생할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxmZeEMjYFi4"
   },
   "source": [
    "**LSTM**: 장기 의존성 문제를 해결하기 위해 어떤 inductive bias가 추가되는가?  \n",
    "답변 :\n",
    "- Inductive Bias: 장기 의존성을 효과적으로 처리할 수 있도록 게이트 메커니즘(입력 게이트, 망각 게이트, 출력 게이트)을 추가함.\n",
    "- 해결하는 문제: 기존 RNN의 장기 의존성 문제를 해결하여 긴 문맥 정보도 학습할 수 있도록 유도함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0beew9RoYKoG"
   },
   "source": [
    "**Attention**: 문장 내 특정 단어에 집중하는 Attention 메커니즘의 inductive bias는 어떤 특징이 있는가?\n",
    "\n",
    "답변 : \n",
    "- Inductive Bias: 모든 입력 토큰이 동일한 중요도를 가지는 것이 아니라, 특정 토큰이 더 중요한 역할을 할 수 있다고 가정함.\n",
    "- 특징: 문장 내에서 특정 단어나 문맥이 더 중요한 경우 가중치를 높여 학습하며, 이는 기계 번역, 문맥 이해, 문서 요약 등에서 효과적임.\n",
    "- Transformer 모델에서 강력한 성능을 발휘하는 이유 중 하나."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z49LCildZuwn"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qcGXdJ_WYcue"
   },
   "source": [
    "# 📖 과제 2 : 네이버 영화 리뷰 감정 분석 (NSMC) 실습  \n",
    "\n",
    "## 📌 1. 실습 개요  \n",
    "이 실습에서는 **네이버 영화 리뷰 감정 분석 데이터(NSMC)**를 사용하여 텍스트 감정을 분류하는 **딥러닝 모델(RNN, LSTM, RNN+Attention)**을 학습합니다.  \n",
    "각 모델의 성능을 비교하고, **사용자 입력 문장에 대한 감정 예측**을 수행합니다.  \n",
    "\n",
    "\n",
    "## 🎯 2. 실습 목표  \n",
    "✔ 감정 분석을 위한 데이터 전처리 및 모델 학습 이해  \n",
    "✔ RNN, LSTM, Attention을 활용한 감정 분석 모델 구현 및 성능 비교  \n",
    "✔ 학습한 모델이 실제 문장을 어떻게 분류하는지 확인  \n",
    "\n",
    "### GPU 사용 권장! (ex, colab의 T4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RRW_OwxMRCPR"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 0. 기본 설치 (Colab 환경)\n",
    "############################\n",
    "\n",
    "# Colab 등에서 한국어 형태소 분석기 사용 위해 konlpy 설치\n",
    "\n",
    " !pip install konlpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6w0uVwjURDFF"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 1. 라이브러리 임포트\n",
    "############################\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import urllib.request\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# GPU 사용 가능 여부\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 한국어 형태소 분석기 (예: Okt 사용)\n",
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Luv3lW5SRG3e"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 2. NSMC 데이터 다운로드\n",
    "############################\n",
    "\n",
    "# ratings_train.txt, ratings_test.txt 파일이 없다면 다운로드\n",
    "# (Colab 환경에서 예시)\n",
    "if not os.path.isfile(\"ratings_train.txt\"):\n",
    "    url_train = \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_train.txt\"\n",
    "    urllib.request.urlretrieve(url_train, \"ratings_train.txt\")\n",
    "\n",
    "if not os.path.isfile(\"ratings_test.txt\"):\n",
    "    url_test = \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings_test.txt\"\n",
    "    urllib.request.urlretrieve(url_test, \"ratings_test.txt\")\n",
    "\n",
    "print(\"NSMC 데이터 다운로드 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0MRBJyVHRIqH"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 3. 데이터 로드 및 전처리\n",
    "############################\n",
    "\n",
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    ratings_train.txt / ratings_test.txt 파일을 불러와서\n",
    "    [text, label] 형태의 리스트를 반환\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        next(f)  # 헤더 건너뛰기\n",
    "        for line in f:\n",
    "            id_, text, label = line.strip().split('\\t')\n",
    "            if text == \"\":  # 공백 리뷰 제거\n",
    "                continue\n",
    "            data.append((text, int(label)))\n",
    "    return data\n",
    "\n",
    "train_data_raw = load_data(\"ratings_train.txt\")\n",
    "test_data_raw = load_data(\"ratings_test.txt\")\n",
    "\n",
    "print(\"훈련 데이터 개수:\", len(train_data_raw))\n",
    "print(\"테스트 데이터 개수:\", len(test_data_raw))\n",
    "\n",
    "# 데이터 샘플 확인\n",
    "print(\"\\n첫 번째 훈련 샘플:\", train_data_raw[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CIVZhY5dRKPi"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 3-1. 형태소 분석 & 토큰화\n",
    "############################\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    간단히 Okt 형태소 분석기로 명사, 동사 등 단어 단위 토큰화 예시.\n",
    "    필요에 따라 불용어 제거 등의 추가 처리가 가능.\n",
    "    \"\"\"\n",
    "    # 예시: 불필요한 기호 제거 (정규 표현식 등)\n",
    "    # 여기서는 간단히 한글, 영문, 숫자, 공백, 일부 문장부호만 남긴다고 가정\n",
    "    import re\n",
    "    text = re.sub(r\"[^가-힣0-9a-zA-Z\\s?!.,]\", \"\", text)\n",
    "\n",
    "    # 형태소 분석(Okt)\n",
    "    # 이 예시에선 그냥 morphemes만 추출\n",
    "    tokens = okt.morphs(text.strip())\n",
    "    return tokens\n",
    "\n",
    "# 간단 테스트\n",
    "sample_text = train_data_raw[0][0]\n",
    "print(\"원문:\", sample_text)\n",
    "print(\"토큰화 결과:\", tokenize(sample_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rv0mQD_JRL4A"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 3-2. 어휘 사전(Vocabulary) 생성\n",
    "############################\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# 매우 큰 데이터를 모두 사용할 경우 시간이 오래 걸릴 수 있으므로,\n",
    "# 우선 \"일부\" 데이터(예: 상위 N개)만 사용해 시범실습할 수도 있습니다.\n",
    "# 필요시 full_data = train_data_raw 로 변경하세요.\n",
    "N = 20000  # 시범으로 2만 개만 사용할 경우\n",
    "full_data = train_data_raw[:N]\n",
    "\n",
    "word_freq = defaultdict(int)\n",
    "\n",
    "for text, label in full_data:\n",
    "    tokens = tokenize(text)\n",
    "    for tok in tokens:\n",
    "        word_freq[tok] += 1\n",
    "\n",
    "# 사용 빈도가 낮은 단어는 제외 (예: 3회 미만)\n",
    "min_freq = 3\n",
    "vocab = [\"<pad>\", \"<unk>\"]\n",
    "for w, f in word_freq.items():\n",
    "    if f >= min_freq:\n",
    "        vocab.append(w)\n",
    "\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "print(\"단어 사전 크기:\", len(vocab))\n",
    "\n",
    "def text_to_ids(text, word2idx):\n",
    "    tokens = tokenize(text)\n",
    "    ids = []\n",
    "    for tok in tokens:\n",
    "        ids.append(word2idx.get(tok, word2idx[\"<unk>\"]))\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mf8dSkbrRNoR"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 4. Dataset 및 DataLoader 구성\n",
    "############################\n",
    "\n",
    "# 최대 문장 길이(패딩을 위한). 너무 크게 잡으면 메모리 사용량 증가\n",
    "max_len = 30\n",
    "\n",
    "def pad_sequences(seq, max_len):\n",
    "    \"\"\"\n",
    "    입력된 seq (List of token ids)에 대해\n",
    "    max_len만큼 뒤를 <pad> 토큰으로 채워넣거나 자르는 함수\n",
    "    \"\"\"\n",
    "    if len(seq) < max_len:\n",
    "        seq = seq + [word2idx[\"<pad>\"]] * (max_len - len(seq))\n",
    "    else:\n",
    "        seq = seq[:max_len]\n",
    "    return seq\n",
    "\n",
    "class NSMCDataset(Dataset):\n",
    "    def __init__(self, data, word2idx, max_len=30):\n",
    "        self.data = data\n",
    "        self.word2idx = word2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, label = self.data[idx]\n",
    "        token_ids = text_to_ids(text, self.word2idx)\n",
    "        token_ids = pad_sequences(token_ids, self.max_len)\n",
    "        return torch.LongTensor(token_ids), torch.LongTensor([label])\n",
    "\n",
    "# 실제로는 전 데이터 쓰려면 full_data = train_data_raw, etc.\n",
    "# 여기서는 위에서 N=20000으로 제한한 데이터 사용 (시간 단축용)\n",
    "train_dataset = NSMCDataset(full_data, word2idx, max_len=max_len)\n",
    "test_dataset = NSMCDataset(test_data_raw, word2idx, max_len=max_len)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"훈련 배치 개수: {len(train_loader)}\")\n",
    "print(f\"테스트 배치 개수: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0sH1Rzo9RO9b"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 4. Dataset 및 DataLoader 구성\n",
    "############################\n",
    "\n",
    "# 최대 문장 길이(패딩을 위한). 너무 크게 잡으면 메모리 사용량 증가\n",
    "max_len = 30\n",
    "\n",
    "def pad_sequences(seq, max_len):\n",
    "    \"\"\"\n",
    "    입력된 seq (List of token ids)에 대해\n",
    "    max_len만큼 뒤를 <pad> 토큰으로 채워넣거나 자르는 함수\n",
    "    \"\"\"\n",
    "    if len(seq) < max_len:\n",
    "        seq = seq + [word2idx[\"<pad>\"]] * (max_len - len(seq))\n",
    "    else:\n",
    "        seq = seq[:max_len]\n",
    "    return seq\n",
    "\n",
    "class NSMCDataset(Dataset):\n",
    "    def __init__(self, data, word2idx, max_len=30):\n",
    "        self.data = data\n",
    "        self.word2idx = word2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, label = self.data[idx]\n",
    "        token_ids = text_to_ids(text, self.word2idx)\n",
    "        token_ids = pad_sequences(token_ids, self.max_len)\n",
    "        return torch.LongTensor(token_ids), torch.LongTensor([label])\n",
    "\n",
    "# 실제로는 전 데이터 쓰려면 full_data = train_data_raw, etc.\n",
    "# 여기서는 위에서 N=20000으로 제한한 데이터 사용 (시간 단축용)\n",
    "train_dataset = NSMCDataset(full_data, word2idx, max_len=max_len)\n",
    "test_dataset = NSMCDataset(test_data_raw, word2idx, max_len=max_len)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"훈련 배치 개수: {len(train_loader)}\")\n",
    "print(f\"테스트 배치 개수: {len(test_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DcKYZHaRRIC"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 5. 모델 정의\n",
    "#   5-1) RNN Classifier\n",
    "############################\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes=2):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.rnn = nn.RNN(__________, __________, batch_first=True)  # (힌트: 입력 차원은 단어 임베딩 차원, 은닉 상태 차원은 hidden_dim)\n",
    "        self.fc = nn.Linear(__________, __________)  # (힌트: 마지막 은닉 상태 크기와 출력 클래스 개수)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len)\n",
    "        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        output, hidden = self.rnn(__________)  # (힌트: RNN의 입력은 임베딩된 단어 벡터)\n",
    "        last_hidden = hidden[__________]  # (힌트: RNN의 마지막 은닉 상태를 가져오기, 0번째 인덱스 사용)\n",
    "        logits = self.fc(__________)  # (힌트: 최종 은닉 상태를 FC Layer에 통과시켜 감정 예측)\n",
    "        return logits\n",
    "\n",
    "############################\n",
    "#   5-2) LSTM Classifier\n",
    "############################\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes=2):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(__________, __________)  # (힌트: 단어 사전 크기와 임베딩 차원 입력)\n",
    "        self.lstm = nn.LSTM(__________, __________, batch_first=True)  # (힌트: LSTM의 입력 차원은 embed_dim, 출력 차원은 hidden_dim)\n",
    "        self.fc = nn.Linear(__________, __________)  # (힌트: 마지막 은닉 상태 크기와 출력 클래스 개수)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        last_hidden = hidden[__________]  # (힌트: LSTM의 마지막 hidden state 사용, 0번째 인덱스)\n",
    "        logits = self.fc(__________)  # (힌트: 마지막 은닉 상태를 FC Layer에 통과시켜 감정 예측)\n",
    "        return logits\n",
    "\n",
    "############################\n",
    "#   5-3) RNN + Attention\n",
    "############################\n",
    "\n",
    "class RNNWithAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes=2):\n",
    "        super(RNNWithAttention, self).__init__()\n",
    "        self.embedding = nn.Embedding(__________, __________)  # (힌트: 단어 사전 크기와 임베딩 차원 입력)\n",
    "        self.rnn = nn.RNN(__________, __________, batch_first=True)  # (힌트: RNN의 입력 차원은 embed_dim, 출력 차원은 hidden_dim)\n",
    "        self.attn_fc = nn.Linear(__________, __________)  # (힌트: attention 가중치를 학습하는 FC Layer, 입력은 hidden_dim)\n",
    "        self.fc = nn.Linear(__________, __________)  # (힌트: 최종 context 벡터를 감정 분류하는 FC Layer에 입력)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
    "        output, hidden = self.rnn(embedded)  # (batch, seq_len, hidden_dim)\n",
    "\n",
    "        # 스코어 계산\n",
    "        score = torch.tanh(self.attn_fc(__________))  # (힌트: output을 attention layer에 통과)\n",
    "\n",
    "        # hidden_dim -> 1 로 축소하여 각 타임스텝별 스칼라 스코어 얻기\n",
    "        score = torch.sum(score, dim=__________)  # (힌트: hidden_dim 차원 축소, dim=2)\n",
    "\n",
    "        # softmax로 가중치 계산\n",
    "        attn_weights = F.softmax(score, dim=__________).unsqueeze(-1)  # (힌트: 시퀀스 차원에 대해 softmax 적용, dim=1)\n",
    "\n",
    "        # 가중합\n",
    "        context = output * attn_weights  # (batch, seq_len, hidden_dim)\n",
    "        context = torch.sum(context, dim=__________)  # (힌트: 타임스텝 차원을 축소하여 문장 단위 벡터 얻기, dim=1)\n",
    "\n",
    "        logits = self.fc(__________)  # (힌트: context 벡터를 FC Layer에 입력하여 감정 분류)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zEx46AkTRTS5"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 6. 학습 / 평가 루프 정의\n",
    "############################\n",
    "\n",
    "def train_model(model, train_loader, val_loader=None, epochs=5, lr=0.001):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)      # (batch, seq_len)\n",
    "            labels = labels.squeeze().to(device)  # (batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)         # (batch, num_classes)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # 에폭별 평균 로스\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        # 검증 정확도(옵션)\n",
    "        if val_loader is not None:\n",
    "            acc = evaluate_model(model, val_loader)\n",
    "            val_accuracies.append(acc)\n",
    "            print(f\"[Epoch {epoch}] Train Loss: {epoch_loss:.4f}, Val Acc: {acc*100:.2f}%\")\n",
    "        else:\n",
    "            print(f\"[Epoch {epoch}] Train Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    return train_losses, val_accuracies\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (inputs, labels) in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.squeeze().to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy().tolist())\n",
    "            all_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    return acc\n",
    "\n",
    "def plot_training_curve(train_losses, val_accuracies=None, title=\"Training Curve\"):\n",
    "    plt.figure(figsize=(10,4))\n",
    "\n",
    "    # 학습 손실 곡선\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.title(\"Train Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # 검증 정확도 곡선\n",
    "    if val_accuracies is not None and len(val_accuracies) > 0:\n",
    "        plt.subplot(1,2,2)\n",
    "        plt.plot(val_accuracies, label='Val Acc', color='orange')\n",
    "        plt.title(\"Validation Accuracy\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "79jQk5VHRU68"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 7. 데이터 분할(Train/Valid) 준비\n",
    "############################\n",
    "\n",
    "# 여기서는 별도의 Validation 세트를 만들지 않고,\n",
    "# 주어진 test_data_raw를 최종 테스트 용도로만 사용할 수도 있음.\n",
    "# 하지만 간단히 train_data_raw를 다시 train/val로 나누는 방법 시연.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 현재 full_data (N=20000) 중 80%: train, 20%: val\n",
    "train_subset, val_subset = train_test_split(full_data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset_2 = NSMCDataset(train_subset, word2idx, max_len)\n",
    "val_dataset_2   = NSMCDataset(val_subset, word2idx, max_len)\n",
    "\n",
    "train_loader_2 = DataLoader(train_dataset_2, batch_size=batch_size, shuffle=True)\n",
    "val_loader_2   = DataLoader(val_dataset_2, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"Train subset: {len(train_subset)}, Val subset: {len(val_subset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dKDag3ORWYP"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 8. 모델별 학습 & 평가\n",
    "############################\n",
    "\n",
    "embed_dim = 128\n",
    "hidden_dim = 128\n",
    "num_classes = 2\n",
    "EPOCHS = 20 # 유동적으로 조절하세요\n",
    "LR = 0.001\n",
    "\n",
    "# --- 8-1. RNN ---\n",
    "print(\"========= RNN Training =========\")\n",
    "rnn_model = RNNClassifier(len(vocab), embed_dim, hidden_dim, num_classes)\n",
    "rnn_train_losses, rnn_val_accs = train_model(rnn_model, train_loader_2, val_loader_2, epochs=EPOCHS, lr=LR)\n",
    "plot_training_curve(rnn_train_losses, rnn_val_accs, title=\"RNN\")\n",
    "\n",
    "# 최종 테스트 세트 정확도\n",
    "rnn_test_acc = evaluate_model(rnn_model, test_loader)\n",
    "print(f\"RNN Test Accuracy: {rnn_test_acc*100:.2f}%\\n\")\n",
    "\n",
    "# --- 8-2. LSTM ---\n",
    "print(\"========= LSTM Training =========\")\n",
    "lstm_model = LSTMClassifier(len(vocab), embed_dim, hidden_dim, num_classes)\n",
    "lstm_train_losses, lstm_val_accs = train_model(lstm_model, train_loader_2, val_loader_2, epochs=EPOCHS, lr=LR)\n",
    "plot_training_curve(lstm_train_losses, lstm_val_accs, title=\"LSTM\")\n",
    "\n",
    "# 최종 테스트 세트 정확도\n",
    "lstm_test_acc = evaluate_model(lstm_model, test_loader)\n",
    "print(f\"LSTM Test Accuracy: {lstm_test_acc*100:.2f}%\\n\")\n",
    "\n",
    "# --- 8-3. RNN + Attention ---\n",
    "print(\"========= RNN + Attention Training =========\")\n",
    "attn_model = RNNWithAttention(len(vocab), embed_dim, hidden_dim, num_classes)\n",
    "attn_train_losses, attn_val_accs = train_model(attn_model, train_loader_2, val_loader_2, epochs=EPOCHS, lr=LR)\n",
    "plot_training_curve(attn_train_losses, attn_val_accs, title=\"RNN+Attention\")\n",
    "\n",
    "# 최종 테스트 세트 정확도\n",
    "attn_test_acc = evaluate_model(attn_model, test_loader)\n",
    "print(f\"RNN+Attention Test Accuracy: {attn_test_acc*100:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eUFqtZw8RYQn"
   },
   "outputs": [],
   "source": [
    "############################\n",
    "# 9. 혼동 행렬(Confusion Matrix) 시각화\n",
    "############################\n",
    "\n",
    "def plot_confusion_matrix(model, data_loader, title=\"Confusion Matrix\"):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for (inputs, labels) in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.squeeze().to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "print(\"=== Confusion Matrix: RNN ===\")\n",
    "plot_confusion_matrix(rnn_model, test_loader, title=\"RNN Confusion Matrix\")\n",
    "\n",
    "print(\"=== Confusion Matrix: LSTM ===\")\n",
    "plot_confusion_matrix(lstm_model, test_loader, title=\"LSTM Confusion Matrix\")\n",
    "\n",
    "print(\"=== Confusion Matrix: RNN+Attention ===\")\n",
    "plot_confusion_matrix(attn_model, test_loader, title=\"RNN+Attention Confusion Matrix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "En4Dc61WRZ7f"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 문장을 모델이 예측할 수 있도록 변환하고 감정 분석 수행\n",
    "def predict_sentiment(model, sentence, word2idx, max_len=30):\n",
    "    \"\"\"\n",
    "    감정 분석을 수행하는 함수 (입력 문장을 숫자 토큰으로 변환 후 모델에 입력)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    tokens = tokenize(sentence)  # 형태소 분석 및 토큰화\n",
    "    token_ids = [word2idx.get(tok, word2idx[\"<unk>\"]) for tok in tokens]  # 단어를 ID로 변환\n",
    "    token_ids = pad_sequences(token_ids, max_len)  # 패딩 적용\n",
    "\n",
    "    input_tensor = torch.LongTensor(token_ids).unsqueeze(0).to(device)  # (1, seq_len)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)  # 모델 예측\n",
    "        probs = F.softmax(output, dim=1)  # 확률 값\n",
    "        pred_label = torch.argmax(probs, dim=1).item()  # 예측 라벨 (0: 부정, 1: 긍정)\n",
    "\n",
    "    return pred_label, probs.squeeze().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y_nz7QRmZVOt"
   },
   "outputs": [],
   "source": [
    "# 예측 함수: 3개 모델 모두 실행\n",
    "def predict_with_all_models(sentence):\n",
    "    \"\"\"\n",
    "    사용자 입력 문장에 대해 RNN, LSTM, RNN+Attention 모델이 각각 어떻게 예측하는지 출력\n",
    "    \"\"\"\n",
    "    print(\"\\n💬 입력 문장:\", sentence)\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    models = {\n",
    "        \"RNN\": rnn_model,\n",
    "        \"LSTM\": lstm_model,\n",
    "        \"RNN+Attention\": attn_model\n",
    "    }\n",
    "\n",
    "    for model_name, model in models.items():\n",
    "        label, probs = predict_sentiment(model, sentence, word2idx)\n",
    "        sentiment = \"긍정 😊\" if label == 1 else \"부정 😡\"\n",
    "\n",
    "        print(f\"🧠 [{model_name} 모델]\")\n",
    "        print(f\"🔍 예측 감정: {sentiment}\")\n",
    "        print(f\"📊 확률: 부정 {probs[0]:.4f} | 긍정 {probs[1]:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# 사용자 입력 받기\n",
    "sample_sentence = input(\"문장을 입력하세요: \")\n",
    "predict_with_all_models(sample_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 안돌아감 ... "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
